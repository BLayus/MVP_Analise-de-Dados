<h1>MVP An√°lise de Dados</h1> <!-- maior -->
<br>
<br>

üìä  [**Acesse aqui o Notebook Completo**](https://github.com/BLayus/MVP_Analise-de-Dados/blob/main/MVP_Analise_de_Dados_Credit_Card_Fraud_Detection.ipynb)
<br>
Estudo completo de an√°lise de dados, pr√©-processamento e modelagem para detec√ß√£o de fraudes usando Python + Scikit-learn

<br>
<br>
<p align="center">
<img src="https://github.com/BLayus/MVP_Analise-de-Dados/blob/main/Img/fraude_ilustr.jpg" 
     alt="Ilustra√ß√£o Fraude" 
     width= "200px"/>
</p>
<br>
<br>
<h2>Projeto de An√°lise e Detec√ß√£o de Fraudes em Compras Online com Cart√£o de Cr√©dito</h2>
<br>
<br>
  
**1. INTRODU√á√ÉO**
1. Quais os desafios que este projeto pretende trabalhar?	
  
**2. ORIGEM E DICION√ÅRIO DOS DADOS**
1. Origem dos dados
2. Dicion√°rio de dados

**3. HIP√ìTESES E PERGUNTAS DE NEG√ìCIO**
1. Quais as perguntas que este projeto pretende responder?

**4. TRATAMENTO INICIAL DOS DADOS**
1. Tipos de dados
2. Nulos
3. Ordena√ß√£o das colunas

**5. AN√ÅLISE DAS VARI√ÅVEIS**
1. Distribui√ß√£o da Vari√°vel Alvo: Fraude
2. An√°lise da Vari√°vel Pa√≠s
3. Avalia√ß√£o da Cardinalidade das Colunas
4. An√°lise de Categorias e Produtos
5. An√°lise do Valor da Compra
6. An√°lise da Coluna Entrega de Documento
7. An√°lise da Data da Compra
8. An√°lise das Colunas Score_1 a Score_10
9. Correla√ß√µes

**6. PR√â PROCESSAMENTO DE DADOS**
1. Divis√£o (Split) dos Dados em Treino e Teste
2. Definir Fun√ß√µes de Pr√©-Processamento
3. Imputers
4. Encoders
5. Normaliza√ß√£o / Escala dos Dados
6. Pipeline

**7. RESPOSTAS √ÄS PERGUNTAS E HIP√ìTESES INICIAIS**

**8. AUTOAVALIA√á√ÉO**

**9. PR√ìXIMOS PASSOS**

**10. CONCLUS√ÉO**
<br>
<br>

>
>
>**1\. Introdu√ß√£o**
>
>
<br>

> 1.1\. Quais os desafios que este projeto pretende trabalhar?
>
<br>

Este Projeto √© o produto principal da Sprint de An√°lise de Dados e Boas Pr√°ticas, segunda etapa do curso de P√≥s Gradua√ß√£o da PUC Rio sobre Data Science and Data Analytics. A proposta deste projeto √© desenvolver uma An√°lise Explorat√≥ria de Dados, (EDA, em ingl√™s), utilizando as ferramentas de programa√ß√£o python, visualiza√ß√£o de dados e boas pr√°ticas de engenharia de software, disciplinas que foram trabalhadas ao longo desta Sprint.

Neste projeto escolhi fazer uma an√°lise de detec√ß√£o de fraudes em compras com cart√£o de cr√©dito em um sistema de e-commerce. Estes dados compreendem um per√≠odo de pouco mais de 1 m√™s entre mar√ßo e abril de 2020 e relata transa√ß√µes de compra de produtos divididos em categorias, data e valor da compra e colunas com dados de score dos clientes, que foram provavelmente adquiridas de bureaus de cr√©dito, junto com algumas outras informa√ß√µes.

Preven√ß√£o √† fraude √© um tema muito importante no universo de Machine Learning, pois algoritmos bem projetados e implementados s√£o capazes de monitorar os milh√µes de transa√ß√µes di√°rias que acontecem no varejo f√≠sico e digital e identificar anomalias e transa√ß√µes fora do padr√£o, disparando alertas para barrar transa√ß√µes suspeitas ou liberando aquelas transa√ß√µes leg√≠timas. 

Em 2024, segundo dados da Serasa, foram feitas mais de 1.8 M de tentativas de fraudes no setor banc√°rio e de cart√µes de cr√©dito no Brasil, gerando um preju√≠zo estimado de R$ 3.5 bi de acordo com a ClearSale. O Brasil ocupa o segundo lugar no ranking dos pa√≠ses que mais sofrem com crimes de fraude, com um risco estimado em 14,25, atr√°s apenas da China, com um risco de 14,93. Ou seja, de cada 100 mil transa√ß√µes registradas, h√° um risco de que 14 a 15 dessas opera√ß√µes sejam tentativas de fraude. Agora, em 2025, o  foco destas fraudes est√° sendo dividido entre fraudes a cart√µes de cr√©dito e fraudes ao Pix. No dia anterior ao dia em que escrevo este texto, um ataque hacker conseguiu invadir o sistema do PIX e desviou um valor pr√≥ximo a 1 bi do sistema banc√°rio.

Estima-se que cerca de metade da popula√ß√£o adulta do Brasil tenha sofrido alguma tentativa de fraude nos √∫ltimos cinco anos, enquanto em outros pa√≠ses com menor √≠ndice este n√∫mero gira em torno de 10%. √â importante distinguir a tentativa de fraude da efetiva√ß√£o dela. Em termos percentuais, apenas 2,5 % das tentativas de fraude s√£o bem sucedidas e pouco a  pouco observa-se uma redu√ß√£o nas fraudes, gra√ßas ao letramento digital da popula√ß√£o, o que a torna menos suscet√≠vel a golpes e a mecanismos mais eficientes de controle e detec√ß√£o de fraudes.

Olhando para a escala do pa√≠s, h√° uma grande concentra√ß√£o das fraudes nos estados de S√£o Paulo, mais de 40%, Rio de Janeiro, com 16% do total de tentativas de fraude, seguido por Minas Gerais e Paran√°, com 8% e 6%, respectivamente.
<br>
<br>

>
>
>**2\. Origem e Dicion√°rio dos Dados**
>
>
<br>

> 2.1\. Origem dos Dados
>
<br>

Estes dados foram disponibilizados para um case pr√°tico do PED, da professora Renata Biaggi, preparat√≥rio para entrevistas de dados, programa do qual participei em 2023 e 2024. A Renata foi cientista de dados no Mercado livre e provavelmente conseguiu obter com eles esta pequena amostra de dados para estudo de fraudes em compras. 

Este dataset cont√©m 150 mil transa√ß√µes em um per√≠odo de tempo compreendido entre mar√ßo e abril de 2020, em pleno in√≠cio da pandemia de Covid-19. Das 150 mil transa√ß√µes, 7.5 mil s√£o fraudes, portanto 5% dos casos. Este n√∫mero redondo de transa√ß√µes e de fraudes e a propor√ß√£o perfeita (95, 5%), provavelmente indica que este dataset foi preparado e manipulado para esta finalidade de estudo, o que n√£o diminui o sentido did√°tico deste conjunto de dados.

Este conjunto de dados j√° apresenta uma coluna score_fraude_modelo que √© uma probabilidade da compra ser fraudada dada por um modelo legado, j√° existente, por√©m que se mostrava pouco eficiente em discriminar corretamente as compras leg√≠timas das compras fraudadas. O objetivo deste estudo √© apresentar esta probabilidade do modelo legado √© superar este benchmarking, esta linha base.
<br>

> 2.2\. Dicion√°rio de Dados
>
<br>
<div style="text-align: center;">
  <img src="https://github.com/BLayus/MVP_Analise-de-Dados/blob/main/Img/Tabela_dic_dados.JPG"/>
  <br>
</div>
<br>
<br>

>
>
>**3\. Hip√≥teses e Perguntas de Neg√≥cio**
>
>
<br>

>3.1\. Quais as perguntas que este projeto pretewnde responder?
>
<br>
  * Qual o perfil das compras fraudadas? Elas ocorrem preferencialmente em quais hor√°rios, dias da semana, com quais valores e com quais tipos de produtos.

  * Quais m√©tricas podemos elaborar e utilizar para monitorar os preju√≠zos com fraudes?

  * Quais as principais caracter√≠sticas deste dataset que mais se relacionam √†s compras leg√≠timas e fraudadas?

  * A partir destas informa√ß√µes, √© poss√≠vel esbo√ßar um perfil do fraudador?

  * Quais seriam as estrat√©gias e quais modelos ideais de Machine Learning que poderiam contribuir para a diminui√ß√£o de fraudes e como implant√°-las?

  * Qual seria o melhor modelo de Machine Learning para lidar com este problema de classifica√ß√£o? Seriam modelos do tipo ensemble?

  * Estrat√©gias de balanceamento dos dados do tipo Smote (oversampling, undersampling) seriam positivas para a modelagem?
<br>
<br>

>
>
>**4\. Tratamento Inicial dos Dados**
>
>
<br>

>4.1\. Tipos de Dados
>
<br>

A primeira etapa ap√≥s abrir o dataset √© verificar o n√∫mero de linhas e colunas e as principais informa√ß√µes dos dados, como data types (tipos de dados) e a exist√™ncia de nulos. Uma primeira checagem indica que os dados est√£o relativamente bons para uso: os tipos n√£o est√£o t√£o absurdos, aparentemente possuem poucos nulos e os nomes de colunas est√£o devidamente grafados com letras em caixa baixa e underscores entre palavras. Tamb√©m constata-se que o tamanho do arquivo √© de aproximadamente 23 MB de mem√≥ria, o que o torna facilmente manipul√°vel no Google Colab.
<br>

>4.2\. Nulos
>
<br>

Ainda atrav√©s do m√©todo df.info(), √© poss√≠vel ver a contagem de nulos e o valor total de linhas de dados presentes no dataset. Sistemas de Machine Learning (ML), na maioria dos casos, n√£o s√£o capazes de lidar com dados faltantes (nulos), por isso a etapa de verifica√ß√£o e tratamento dos valores nulos e posterior imputa√ß√£o / preenchimento ou remo√ß√£o de dados √© fundamental para fazer um pr√© processamento eficiente, preparando o dataset para futuras etapas de modelagem de ML.
<br>

>4.3\. Ordena√ß√£o das Colunas
>
<br>

Ao abrir o dataset, optei por reordenar o index de colunas para deix√°-las organizadas em blocos. O primeiro bloco de colunas s√£o as colunas com scores. O segundo bloco s√£o as colunas de entrega dos documentos. O terceiro bloco s√£o as informa√ß√µes da compra e, por fim, as informa√ß√µes de fraude. 

O gr√°fico abaixo mostra de uma forma visual e organizada onde est√£o distribu√≠dos os dados faltantes ao longo de todo dataset:
<br>
<div style="text-align: center;">
  <img src="https://github.com/BLayus/MVP_Analise-de-Dados/blob/main/Img/Heatmap_Nulos.png"/>
  <br>
</div>
<br>
<br>

>
>
>**5\. An√°lise das Vari√°veis**
>
>
<br>

>5.1\. Distribui√ß√£o da Vari√°vel Alvo: Fraude
>
<br>

A primeira avalia√ß√£o deste conjunto de dados, que busca compreender a din√¢mica das transa√ß√µes fraudadas e compar√°-las √†s transa√ß√µes leg√≠timas, √© justamente olhar para a quantidade absoluta e percentual de fraudes presentes neste dataset.

Esta amostra conta com 150 mil linhas de dados, das quais 7500 foram marcadas como fraudes, perfazendo 5% das transa√ß√µes. Apesar do evidente desbalanceamento, ainda assim, 5% √© um percentual bastante elevado de fraudes. 

Este problema √© um cl√°ssico problema de classifica√ß√£o em Machine Learning, onde o objetivo √© prever se uma transa√ß√£o √© (1) ou n√£o (0) fraudada. Este tipo de classifica√ß√£o √© feita com base na probabilidade de pertencer √† classe minorit√°ria, onde o modelo de ML prev√™ um score para cada transa√ß√£o e acima de um threshold define se aquele caso √© classificado em positivo ou negativo.
<br>
<div style="text-align: center;">
  <img src="https://github.com/BLayus/MVP_Analise-de-Dados/blob/main/Img/Fraudes.png"/>
  <br>
</div>
<br>

>5.2\. An√°lise da Vari√°vel Pa√≠s
>
<br>

A pr√≥xima vari√°vel de interesse √© a coluna Pa√≠s. Sabemos que o Brasil √© um dos campe√µes em fraudes, ser√° que com estes dados iremos confirmar esta afirma√ß√£o? Quantos pa√≠ses aparecem com mais frequ√™ncia neste dataset e onde acontecem mais fraudes? Ser√° que o pa√≠s onde ocorre a transa√ß√£o √© determinante para definirmos o risco de fraude?
<br>
<p align="center">
  <img src="https://github.com/BLayus/MVP_Analise-de-Dados/blob/main/Img/Pareto%20Compras%20Pais.png" height="100px" style="display:inline-block; margin-right:10px;"/>
  <img src="https://github.com/BLayus/MVP_Analise-de-Dados/blob/main/Img/Pareto%20Fraudes%20Pais.png" height="100px" style="display:inline-block;"/>
</p>
<br>
No gr√°fico acima podemos ver, em azul, a quantidade absoluta de compras e, na linha cinza, a curva acumulada para cada pa√≠s. Esta visualiza√ß√£o √© bem interessante pois podemos ver ao mesmo tempo quanto cada pa√≠s representa em compras tanto em termos absolutos quanto em termos percentuais. Podemos ver no gr√°fico da esquerda que o Brasil concentra mais de 100 mil transa√ß√µes, o que equivale a aproximadamente 75% do total. Argentina √© o segundo pa√≠s com mais compras, com aproximadamente 30 mil transa√ß√µes, equivalente a mais de 20% do total. Os dois pa√≠ses, Brasil e Argentina, juntos somam mais de 95% de todas as transa√ß√µes. Ao restante dos pa√≠ses sobra uma participa√ß√£o bem pequena neste conjunto de dados.

No gr√°fico da direita, em vermelho vemos o total e o acumulado das transa√ß√µes fraudadas. De novo, o Brasil √© o pa√≠s que mais possui compras fraudadas, com mais de 6 mil transa√ß√µes, o que equivale a mais de 82%. Argentina tamb√©m √© o segundo pa√≠s com mais fraudes, aproximadamente 1 mil transa√ß√µes, o que equivale a mais de 15% das fraudes. Os dois pa√≠ses juntos respondem por mais de 97.5% das transa√ß√µes fraudadas.
<br>

>5.3\. Avaliac√£o da Cardinalidade das Colunas
>
<br>

Analisar a cardinalidade, ou seja, quantos valores diferentes cada coluna apresenta, √© uma etapa importante da an√°lise de dados, pois √© poss√≠vel, atrav√©s da cardinalidade, fazer algumas infer√™ncias a respeito das vari√°veis e quanto elas podem ou n√£o contribuir para uma futura modelagem de ML.

Por exemplo, a coluna Score_1, aparentemente √© uma coluna num√©rica, mas ao olharmos a cardinalidade dela, percebe-se que s√≥ h√° 4 valores distintos. Logo podemos afirmar que, na verdade, este tipo de dado √© mais categ√≥rico do que num√©rico. Talvez as colunas Score_4 e Score_7, que possuem 51 e  59 valores distintos, respectivamente, tamb√©m possam ser consideradas como categorias, apesar de serem colunas com valores num√©ricos discretos. Por√©m, como n√£o temos mais nenhuma informa√ß√£o destas 2 √∫ltimas colunas score, talvez seja melhor mant√™-las como est√£o.

Tamb√©m vemos uma cardinalidade muito alta na coluna produto, com mais de 127 mil valores diferentes, o que reflete a grande diversidade de itens do varejo digital. A coluna categoria_produto apresenta aproximadamente 8300 diferentes valores, mas ser√° que h√° alguma rela√ß√£o entre fraudes e categorias? E rela√ß√£o entre fraude e tipo de produto?
<br>


<div style="text-align: center;">
  <img src="https://github.com/BLayus/MVP_Analise-de-Dados/blob/main/Img/Acumulado%20categorias.png"/>
  <br>
</div>
<br>

Apesar de n√£o sabermos o que estes c√≥digos das categorias representam, podemos perceber que h√° algumas categorias com maior concentra√ß√£o de fraudes do que outras. Neste gr√°fico acima, podemos ver como as 10 categorias com mais fraudes concentram aproximadamente 15% do total e que a primeira categoria mais fraudada √© visivelmente mais atingida por fraudes do que as outras. Vamos olhar esta categoria em detalhe e ver qual tipo de produtos ela agrega.

Tamb√©m podemos constatar, atrav√©s do gr√°fico da direita, que quase 80% das categorias n√£o apresentam nenhuma ocorr√™ncia de fraude, enquanto aproximadamente 20% das categorias concentram todas as transa√ß√µes fraudadas deste conjunto de dados.

Ao filtrar os dados pela categoria mais fraudada e agrupar por produto, podemos ver claramente que h√° uma predile√ß√£o por comprar aparelhos celulares em transa√ß√µes fraudadas. Isso parece bastante curioso e informativo para tra√ßar o perfil do fraudador.
<br>
<div style="text-align: center;">
  <img src="https://github.com/BLayus/MVP_Analise-de-Dados/blob/main/Img/Barra%20comparativa%20Categorias.png"/>
  <br>
</div>
<br>

>5.5\. An√°lise do Valor da Compra
>
<br>

Na coluna valor da compra, muitas informa√ß√µes podem ser extra√≠das, j√° que este √© uma vari√°vel de vital import√¢ncia para todo com√©rcio, seja ele f√≠sico ou digital. A companhia de cart√µes de cr√©dito obt√©m um ganho de 10% do valor de cada compra leg√≠tima, por√©m perde o valor integral da transa√ß√£o se a compra for aprovada e se revelar uma fraude. √â do valor das compras que a empresa obt√©m sua receita e os vendedores tamb√©m, portanto este √© o grande prop√≥sito de toda a opera√ß√£o.

Em primeiro lugar, podemos olhar para algumas m√©tricas que resumem a situa√ß√£o das compras, tanto leg√≠timas, quanto fraudadas. Depois veremos alguns gr√°ficos mostrando as faixas de valores mais compradas e mais fraudadas, por fim veremos em quais faixas de pre√ßo a companhia tem lucro ou preju√≠zo com as compras e fraudes.
<br>

<div style="text-align: center;">
  <img src="https://github.com/BLayus/MVP_Analise-de-Dados/blob/main/Img/Tabela_metricasas.JPG"/>
  <br>
</div>
<br>

Da tabela acima, constatamos que, apesar da movimenta√ß√£o total no per√≠odo ser de aproximadamente 6 MI e a companhia faturar 10% deste valor (aprox. 598 K), o preju√≠zo causado por fraudes foi de 547 K. Descontado o preju√≠zo causado por fraudes, o lucro da opera√ß√£o foi de apenas algo em torno de 50 K, menos de 1% do total movimentado. Esses valores oferecem uma perspectiva bem preocupante do tamanho do problema causado por fraudes.
<br>

<div style="text-align: center;">
  <img src="https://github.com/BLayus/MVP_Analise-de-Dados/blob/main/Img/Graficos%20valor%20compra.png"/>
  <br>
</div>
<br>

Este par de gr√°ficos mostra a quantidade absoluta e percentual das compras leg√≠timas e fraudadas em rela√ß√£o √†s faixas de valor em que ocorrem. Do gr√°fico √† esquerda, reparar como a maioria das compras (a linha vermelha tracejada indica o corte em 95% das compras) acontecem na faixa de valores abaixo de R$ 200, 99% das compras acontecem abaixo de R$ 500. Apenas 1% de todas as transa√ß√µes s√£o acima desta faixa de R$ 500, o que resulta em um ticket m√©dio muito baixo. 

Pelo gr√°fico √† direita, por outro lado, podemos ver como o valor de corte dos 95% das fraudes est√° em quase R$ 500 e 2% das fraudes ocorrem em valores acima destes R$ 500, o que indica um ‚Äúticket m√©dio‚Äùdas fraudes mais alto do que o das compras leg√≠timas. Vamos ver no pr√≥ximo par de gr√°ficos a mesma rela√ß√£o, s√≥ que desta vez pensando no valor total das transa√ß√µes. 

Do gr√°fico √† esquerda, podemos ver como dos quase R$ 6 MI em  transa√ß√µes, os 95%, aproximadamente R$ 5 MI acumulados foram de compras feitas com valores abaixo de R$ 900, onde est√° a maior fatia do mercado e do faturamento.

Do gr√°fico √† direita, podemos ver como os 95% acumulados no valor total de fraudes acontece na faixa at√© R$ 2K. As fraudes se distribuem por uma faixa mais ampla de valores e tamb√©m acontecem com mais frequ√™ncia em faixas de valores mais altos.
<br>

<div style="text-align: center;">
  <img src="https://github.com/BLayus/MVP_Analise-de-Dados/blob/main/Img/Graficos%20valor%20total%20compra.png"/>
  <br>
</div>
<br>

<div style="text-align: center;">
  <img src="https://github.com/BLayus/MVP_Analise-de-Dados/blob/main/Img/Lucro%20Prejuizo.png"/>
  <br>
</div>
<br>

Este gr√°fico acima elucida as faixas de pre√ßo onde a companhia obteve receita e as faixas de pre√ßo onde as perdas por fraude superaram as receitas. 
Impressionante ver como em todas as faixas de pre√ßo, com exce√ß√£o da primeira faixa, as transa√ß√µes fraudadas superaram as receitas. No c√¥mputo geral, este per√≠odo foi muito ruim para a empresa que ficou com uma fatia muito pequena do valor das transa√ß√µes, trabalhando com uma lucratividade muito baixa. Se tiv√©ssemos os dados das despesas da empresa para manter a opera√ß√£o funcionando (aluguel, sal√°rios, impostos, etc), com certeza ir√≠amos confirmar que a empresa estava, neste per√≠odo, operando no preju√≠zo.
<br>

>5.6\. An√°lise da Coluna Entrega de Documento
>
<br>

A entrega ou a falta de entrega dos documentos est√° associada ao cometimento de fraudes. Como podemos ver nas colunas da esquerda, quando os documentos n√£o foram entregues h√° uma maior quantidade de fraudes cometidas e quando os documentos s√£o entregues, a associa√ß√£o com fraudes diminui. Sobretudo o gr√°fico do meio, que mostra a entrega do documento 2, possui um percentual maior de transa√ß√µes fraudadas, com cerca de 20% delas.
<br>
<div style="text-align: center;">
  <img src="https://github.com/BLayus/MVP_Analise-de-Dados/blob/main/Img/doc_1png.png"/>
  <br>
</div>
<br>

<div style="text-align: center;">
  <img src="https://github.com/BLayus/MVP_Analise-de-Dados/blob/main/Img/Doc_2.png"/>
  <br>
</div>
<br>

<div style="text-align: center;">
  <img src="https://github.com/BLayus/MVP_Analise-de-Dados/blob/main/Img/Doc_3.png"/>
  <br>
</div>
<br>

>5.7\. An√°lise da Data da Compra
>
<br>

A inten√ß√£o de analisar esta coluna √© entender se h√° alguma associa√ß√£o significativa entre o per√≠odo do m√™s, dia ou hor√°rio do dia em que mais fraudes acontecem. Esta vari√°vel pode ajudar a construir o perfil do fraudador, por√©m √© dif√≠cil definir com precis√£o se este perfil tra√ßado pela an√°lise ir√° se manter ao longo do tempo. O fraudador, como criminoso que √©, est√° sempre buscando formas de disfar√ßar sua atividade il√≠cita e mudando suas estrat√©gias de atua√ß√£o para tentar passar despercebido. Desta forma, se nos basearmos muito em um perfil comportamental relacionado √† vari√°vel temporal, por exemplo, bloqueando transa√ß√µes em determinado dia ou hora, rapidamente os fraudadores poder√£o perceber estas ‚Äúregras‚Äùe alterar seu modo de atua√ß√£o.

Na hora de preparar o modelo de classifica√ß√£o, a vari√°vel data n√£o dever√° compor o modelo, mas  para fins de an√°lise ela pode ser muito √∫til e trazer informa√ß√µes ricas para compor a interpreta√ß√£o deste tema complexo. Abaixo, vamos ver a distribui√ß√£o por dia:
<br>

<div style="text-align: center;">
  <img src="https://github.com/BLayus/MVP_Analise-de-Dados/blob/main/Img/Compras%20dia%20mes.png"/>
  <br>
</div>
<br>

Este gr√°fico evidencia a distribui√ß√£o das compras leg√≠timas e fraudadas ao longo dos dias do m√™s, com o objetivo de identificar padr√µes e tend√™ncias que possam estar relacionados a maiores √≠ndices de fraudes. Primeiramente, salta aos olhos uma maior concentra√ß√£o de transa√ß√µes na segunda e terceira semanas do m√™s, com uma queda nos dias entre semanas, talvez final de semana, formando um padr√£o em ‚ÄúM‚Äù. As transa√ß√µes fraudadas acompanham esta tend√™ncia, mostrando uma √≥bvia correla√ß√£o positiva entre um maior n√∫mero de transa√ß√µes e um maior n√∫mero de fraudes. Utilizei o teste estat√≠stico de m√©dias Chi-Quadrado para confirmar esta hip√≥tese de que  h√° uma rela√ß√£o estatisticamente significativa entre o dia do m√™s e  a quantidade de fraudes. O  valor chi2 foi de 176.07 e o p- value foi 1.33 e-22, um p-value muito baixo que n√£o rejeita a hip√≥tese nula, ou seja, h√° uma rela√ß√£o estat√≠stica entre dia do m√™s e fraudes.
<br>

<div style="text-align: center;">
  <img src="https://github.com/BLayus/MVP_Analise-de-Dados/blob/main/Img/Compras%20hora%20dia.png"/>
  <br>
</div>
<br>

Um segundo momento de an√°lise pretende verificar se h√° rela√ß√£o associativa entre o hor√°rio do dia e o cometimento de fraudes. Este gr√°fico apresenta uma tend√™ncia de aumento na quantidade de transa√ß√µes durante o per√≠odo comercial e uma diminui√ß√£o durante as madrugadas. As fraudes acompanham o movimento das transa√ß√µes de forma diretamente proporcional ao longo do dia e da noite, mas durante as madrugadas, das 0 horas at√© as 4 ou 5 horas, h√° bem poucas transa√ß√µes e a propor√ß√£o de fraudes aumenta em rela√ß√£o ao total das transa√ß√µes. Tamb√©m realizei um teste de hip√≥tese Chi-quadrado para confirmar esta intui√ß√£o. O resultado do teste chi2 = 841.65 e o p-value = 5.3e-163, confirmam que n√£o √© poss√≠vel aceitar a hip√≥tese nula (de que n√£o h√° rela√ß√£o entre hora do  dia e fraudes), portanto existe sim uma associa√ß√£o estatisticamente significativa entre o hor√°rio e o cometimento de fraudes.
<br>

>5.8\. An√°lise das Colunas Score_1 a Score_10
>
<br>

As colunas Score s√£o dados dos quais n√£o temos nenhuma informa√ß√£o sobre o significado de cada score, portanto s√≥ √© poss√≠vel  avaliar as distribui√ß√µes e tentar extrair alguma associa√ß√£o entre fraudes e cada um deles, bem como averiguar o tipo de vari√°vel (se categ√≥rica, num√©rica, discreta ou cont√≠nua) e procurar por poss√≠veis semelhan√ßas e diferen√ßas nas distribui√ß√µes entre as transa√ß√µes leg√≠timas e fraudadas.
<br>

<div style="text-align: center;">
  <img src="https://github.com/BLayus/MVP_Analise-de-Dados/blob/main/Img/Scores.png"/>
  <br>
</div>
<br>

A coluna Score_1 aparentemente √© uma coluna categ√≥rica, pois s√≥ tem 4 valores distintos, mas h√° uma concentra√ß√£o maior no valor 4, tanto de transa√ß√µes leg√≠timas, quanto fraudadas. O valor 2 tamb√©m apresenta uma concentra√ß√£o alta de dados, principalmente de fraudes. A coluna Score_2 apresenta uma distribui√ß√£o mais pr√≥xima de uma normal, mas com uma cauda √† esquerda, onde tanto as transa√ß√µes leg√≠timas quanto as fraudes se distribuem, de maneira semelhante.

A coluna Score_3 apresenta alta cardinalidade, com mais de 135K valores, distribu√≠dos em um intervalo que varia de 0 a 1.4MI, mas apresenta uma distribui√ß√£o assim√©trica positiva, com uma cauda muito longa e a maioria dos dados em faixas de valores baixos. A coluna Score_4 varia entre 0 e 50 e aparenta uma distribui√ß√£o bimodal, com a maioria dos valores concentrada na parte baixa e na parte alta do intervalo, com o restante dos dados distribu√≠dos mais ou menos uniformemente ao longo do eixo x.

As colunas Score_5 e Score_6 apresentam uma distribui√ß√£o altamente enviesada para a esquerda, com a quase totalidade dos dados na faixa pr√≥xima a zero. As colunas Score_7, Score_9 e Score_10 possuem distribui√ß√µes assim√©tricas positivas (enviesada √† esquerda) com a maioria dos valores concentrados na faixa baixa de valores,  por√© com um decaimento ao longo do intervalo que √†s vezes assume um comportamento exponencial e √†s vezes assume um padr√£o linear.

J√° a coluna Score_8 varia enre 0 e 1 mas possui 150 mil diferentes valores, em uma distribui√ß√£o quase uniformemente distribu√≠da. Parece que √© uma coluna que j√° foi normalizada e que possui um √∫nico valor para cada usu√°rio ou transa√ß√£o. Desta forma ela n√£o acrescena informa√ß√£o de tend√™ncia ou padr√£o, funciona quase como um ID da transa√ß√£o.
<br>
 
>5.9\. Correla√ß√µes
>
<br>

Este dataset possui colunas com informa√ß√µes categ√≥ricas e outras com informa√ß√µes num√©ricas. As correla√ß√µes entre vari√°veis num√©ricas podem ser avaliadas usando a correla√ß√£o de Pearson, uma forma de compara√ß√£o entre m√©dias de vari√°veis que indica a for√ßa e a dire√ß√£o da associa√ß√£o entre duas vari√°veis. Por√©m as vari√°veis categ√≥ricas precisam de outra metodologia de avalia√ß√£o, dependendo do tipo de categoria a que pertencem. Categorias ordinais podem ser avaliadas com a correla√ß√£o de Spearman, baseada em rankeamento (portanto ordem de valor), que indica a for√ßa e dire√ß√£o da associa√ß√£o entre vari√°veis. A correla√ß√£o Ponto Bi-Serial √© uma forma de calcular a associa√ß√£o entre uma vari√°vel bin√°ria (no nosso caso fraude= 1, n√£o fraude = 0) e vari√°veis cont√≠nuas. J√° para avaliar a correla√ß√£o entre duas vari√°veis bin√°rias, podemos usar o teste chi-quadrado e confirmar se h√° ou n√£o uma associa√ß√£o estatisticamente significativa entre duas vari√°veis.

Neste projeto utilizei um mix entre correla√ß√£o de spearman, Bi-Serial e Chi-Quadrado, como podemos ver na figura abaixo:
<br>

<div style="text-align: center;">
  <img src="https://github.com/BLayus/MVP_Analise-de-Dados/blob/main/Img/Matriz%20correlacao.png"/>
  <br>
</div>
<br>

A correla√ß√£o mais forte encontrada neste conjunto de dados √© entre a coluna Score_4 e a coluna Score_10, com um valor positivo de 0.59. Apesar de ser a maior correla√ß√£o identificada, n√£o √© suficientemente alta para precisarmos escolher apenas uma das vari√°veis para o modelo (n√£o recomendado utilizar vari√°veis autocorrelacionadas em ML). O restante das correla√ß√µes n√£o √© significativa, apenas indicam levemente associa√ß√µes entre vari√°veis.

A associa√ß√£o entre as vari√°veis entrega dos documentos 1, 2 e 3 com a vari√°vel fraude foi calculada usando teste chi-quadrado para medir a for√ßa da associa√ß√£o (phi) e o p-value que confirma ou refuta a hip√≥tese de associa√ß√£o. Atrav√©s dos valores calculados e do heatmap, fica confirmado que h√° sim uma associa√ß√£o estatisticamente significativa, ainda que a for√ßa n√£o seja tanta, entre a entrega ou n√£o dos documentos e o cometimento de fraudes.
<br>
<br>


>
>
>**6\. Pr√©-Processamento de Dados**
>
>
<br>

>6.1\. Divis√£o (Split) dos dados em Treino e Teste
>
<br>

Esta etapa de separa√ß√£o de dados para treino e teste √© fundamental para os sistemas de Machine Learning supervisionados possam ‚Äúaprender‚Äù de forma generaliz√°vel e evitar overfitting, ou seja, quando o  modelo v√™ os mesmos exemplos duas vezes e memoriza as respostas ao inv√©s de generalizar para novos dados.

O  conjunto de dados de teste simula dados nunca antes vistos pelo modelo, similar ao que acontece na realidade. A divis√£o entre treino e teste permite comparar o desempenho no treino com o desempenho no teste. Quando h√° um desempenho muito bom no treino e muito ruim no teste, isso pode ser um sinal de pouca capacidade discriminativa do modelo. A divis√£o entre treino e teste tamb√©m √© importante para usar t√©cnicas mais avan√ßadas de valida√ß√£o, como valida√ß√£o cruzada, grid search ou Bayesian search Cross validation. Comparar v√°rias amostras (bootstrap) com cross validation permite uma maior confiabilidade estat√≠stica das m√©tricas de erro usadas para avalia√ß√£o.
<br>

>6.2\. Definir Fun√ß√µes de Pr√©-Processamento
>
<br>

Nesta etapa, o objetivo √© definir fun√ß√µes que possam ser aplicadas de modo generalizado tanto no dataset de treino como no dataset de teste. Ambos os datasets precisam passar exatamente pelo mesmo processo de tratamento e precisam ter os mesmos tipos de dados, ordem, nomes de colunas, escala, etc.

O uso de fun√ß√µes √© uma boa pr√°tica pois permite modularizar o projeto e at√© mesmo utilizar determinadas fun√ß√µes em diferentes projetos. Fun√ß√µes permitem reprodutibilidade e consist√™ncia na forma como os dados s√£o transformados, evita duplica√ß√£o e torna o c√≥digo mais limpo, organizado e f√°cil de fazer altera√ß√µes e manuten√ß√µes.

As fun√ß√µes de pr√©-processamento em um pipeline de dados do sklearn exigem que as transforma√ß√µes sejam encapsuladas em objetos do tipo fit e transform, garantindo que apenas os dados de treino sejam usados para fazer o fit e os dados transformados do conjunto de testes n√£o tenham contato com os dados do conjunto de treino.
<br>

>6.3\. Imputers
>
<br>

Imputers s√£o fun√ß√µes que preenchem os dados nulos ou missing values utilizando crit√©rios para cada tipo de coluna. Constituem uma etapa muito importante pois a maioria dos algoritmos de ML s√£o incapazes de lidar com dados nulos diretamente. A biblioteca sklearn possui a classe SimpleImputer, que pode imputar dados usando estrat√©gias definidas pelo usu√°rio. 

No contexto deste projeto, utilizei imputers para preencher dados faltantes usando a estrat√©gia de imputar valor constante para as categorias pa√≠s e entrega_doc_2 e utilizei a estrat√©gia de imputar a mediana nas colunas score que possu√≠am dados faltantes. Neste √∫ltimo caso, a op√ß√£o pela mediana foi decidida ap√≥s avaliar as distribui√ß√µes das colunas score e constatar que nenhuma delas seguia uma distribui√ß√£o normal.
<br>

>6.4\. Encoders
>
<br>

Encoders s√£o fun√ß√µes que transformam colunas categ√≥ricas em colunas com valores num√©ricos. √â uma etapa importante porque a maioria dos modelos de ML n√£o pode lidar diretamente com textos e categorias n√£o ordinais podem ser compreendidas como ordinais. Por exemplo, neste projeto h√° uma coluna entrega_doc_1 que possui apenas 4 valores num√©ricos (1, 2, 3 e 4), por√©m n√£o temos informa√ß√£o se esta ordena√ß√£o representa um valor da categoria, ou seja, se 4 √© maior que 1, ou se 1 √© melhor do que 4. Por isso a op√ß√£o para esta coluna foi encod√°-la com One Hot Encoding, que ir√° apenas indicar a qual categoria pertence, sem gerar uma ordem de valores. 

O One Hot Encoding (ou encoding  bin√°rio) funciona atribuindo uma nova coluna para cada categoria e marcando todas as linhas que n√£o pertenciam √†quela categoria com o valor 0 e marcando apenas as linhas que eram da categoria original com o valor 1. Ele aumenta a dimensionalidade do dataset, mas √© r√°pido e eficaz de ser aplicado.

Outro encoder utilizado neste projeto foi o WOE, Weight of Element, para encodar a coluna categoria_produto, que √© uma coluna categ√≥rica com alt√≠ssima cardinalidade mais de 8 mil valores diferentes. O WoE funciona calculando para cada categoria a raz√£o entre a quantidade de transa√ß√µes leg√≠timas e quantidade de transa√ß√µes fraudadas e consegue captar bem a rela√ß√£o  entre as categorias que possuem mais e as que possuem menos fraudes.

Outra forma de encoder que este projeto apresenta, derivado do One Hot Encoder, √© criar uma coluna para assinalar a falta de dados nas colunas de entrega de documentos e scores. Neste caso, a premissa adotada foi de que a falta de dados pode ser compreendida como informa√ß√£o com potencial relev√¢ncia para o modelo e que, portanto, deve ser uma informa√ß√£o mantida.
<br>

>6.5\. Normaliza√ß√£o / Escala dos Dados
>
<br>

O prop√≥sito de normalizar os dados √© diminuir o intervalo de valores presentes no conjunto de dados, trazendo os valores para uma escala comum. Modelos de ML baseados em dist√¢ncia e em gradiente s√£o muito sens√≠veis √† escala dos dados. Sem normaliza√ß√£o, as vari√°veis com escalas maiores dominam o modelo e podem distorcer o aprendizado. H√° algumas estrat√©gias de normaliza√ß√£o, mas neste projeto optei por usar a normaliza√ß√£o por Min Max, que mant√©m a distribui√ß√£o original e apenas achata os dados de cada vari√°vel para um intervalo entre 0 e 1. Este scaler foi aplicado a todas as vari√°veis num√©ricas.
<br>

>6.6\. Normaliza√ß√£o / Escala dos Dados
>
<br>

O pipeline √© um dos conceitos mais importantes desta etapa de pr√©-processamento, pois ela ajuda a automatizar e organizar os fluxos do projeto de machine learning. √â uma fun√ß√£o que encadeia e encapsula v√°rias etapas de pr√©-processamento e modelagem em um √∫nico objeto. Neste projeto, para finalizar a etapa de pr√©-processamento, defini uma fun√ß√£o pipeline para encadear todas as etapas anteriores em uma √∫nica. Isso facilita a aplica√ß√£o e teste de diferentes modelos e as etapas de valida√ß√£o cruzada, mantendo a integridade dos conjuntos de treino e teste e a separa√ß√£o correta entre os dados.
<br>
<br>

>
>
>**7\. Respostas √Äs Perguntas e Hip√≥teses Iniciais**
>
>
<br>

Este projeto de An√°lise Explorat√≥ria de Dados, respondeu parte das quest√µes levantadas inicialmente, por√©m este tema de preven√ß√£o √† fraudes √© suficientemente complexo para  restarem muitas d√∫vidas. 

Sobre o perfil das transa√ß√µes fraudadas, acredito que foi poss√≠vel identificar algumas caracter√≠sticas importantes. Transa√ß√µes fraudulentas ocorrem com maior intensidade no Brasil do que em outros pa√≠ses. Fraudes tamb√©m podem ser caracterizadas por preferencialmente acontecerem na compra de produtos da categoria ‚Äú43b9c10‚Äù que engloba certos produtos, como aparelhos de telefonia m√≥vel. Tamb√©m constata-se que as fraudes ocorrem, em m√©dia,  em faixas de valor mais altas do que as compras normais. Tamb√©m vimos que as fraudes ocorrem em maior intensidade durante as madrugadas, mas n√£o s√≥ nestes per√≠odos. 
Fraudes acompanham a quantidade de transa√ß√µes tanto nos dias do m√™s como nos hor√°rios do dia, numa tentativa clara de se misturar entre as compras normais, uma forma de dissimular a atividade il√≠cita. Tamb√©m foi poss√≠vel identificar uma associa√ß√£o entre a n√£o entrega dos documentos, principalmente do tipo 1 e 2, este √∫ltimo concentrando 21% das compras fraudadas entre aqueles que n√£o entregaram a documenta√ß√£o. 

O mercado de cart√µes de cr√©dito  e de transa√ß√µes digitais possui seus pr√≥prios indicadores de monitoramento das transa√ß√µes saud√°veis e fraudadas. Uma delas √© a taxa percentual de fraudes, que neste estudo √© de 5%. Mas neste caso em particular, esta taxa de 5% parece um pouco artificial, o que pode ser explicado pela amostra escolhida, que √© uma fatia temporal de pouco mais de um m√™s no in√≠cio da pandemia, quando todo o  com√©rcio do mundo (e o crime de fraude) se voltou exclusivamente para o meio digital, que pouco a pouco foi desenvolvendo novas formas de preven√ß√£o.  
Outra m√©trica usada √© a taxa de fraudes por cada 100 mil transa√ß√µes, m√©trica que neste caso fica ainda mais inflada, j√° que temos 150 mil dados, das quais 7500 s√£o fraudes. Outra taxa comumente utilizada √© a ‚Äúbasis points , BPS‚Äù, que √© calculada como o valor total das fraudes dividido pelo valor total das transa√ß√µes aprovadas multiplicado por 10000. Esta m√©trica √© mais comum no mercado financeiro e cada basis point representa 0.01%, um cent√©simo de um ponto percentual. Neste dataset, o basis point √© de 914.99, um valor completamente desproporcional, provavelmente causado pela amostra escolhida.

As features que mais se relacionam a fraudes neste dataset s√£o as entregas de documento e a categoria do produto. Como vimos extensamente, a falta de entrega dos documentos, sobretudo o documento tipo 2 √© um ind√≠cio bem forte de possibilidade de fraude. Tamb√©m vimos como h√° uma prefer√™ncia na aquisi√ß√£o de aparelhos celulares em compras fraudadas.

√â bem dif√≠cil tra√ßar um perfil do fraudador com base apenas nas informa√ß√µes deste dataset, pois estes dados n√£o trazem informa√ß√µes a respeito dos clientes, e sim das transa√ß√µes. Caso haja um outro dataset com os ids e informa√ß√µes dos clientes, poder√≠amos cruz√°-lo com estas transa√ß√µes e ent√£o avaliar separadamente o perfil dos clientes envolvidos em fraudes. 

N√£o h√° uma estrat√©gia de preven√ß√£o √† fraudes sozinha que seja suficiente para diminuir as transa√ß√µes falsas. √â necess√°rio implantar um conjunto de medidas e um sistema completo de detec√ß√£o  e preven√ß√£o que passa por mais seguran√ßa no lado do consumidor com senhas e sistemas de dupla ou tripla verifica√ß√£o, alertas em tempo real para que o cliente reconhe√ßa aquela compra e tamb√©m sistemas de machine learning mais precisos para identificar pequenas anomalias e comportamentos fora do normal entre as milh√µes de transa√ß√µes di√°rias. 

O objetivo desta an√°lise √© subsidiar uma pr√≥xima etapa onde ser√° de fato desenvolvido um sistema de machine learning que tenha um desempenho melhor do que o sistema usado nesta empresa dentro deste per√≠odo de mar√ßo a abril de 2020 e que foi respons√°vel por um percentual alto de fraudes que levaram a uma grande perda de faturamento da companhia.
<br>
<br>

>
>
>**8\. Autovalia√ß√£o**
>
>
<br>

Neste projeto fiz uma an√°lise criteriosa das principais vari√°veis envolvidas na classifica√ß√£o de transa√ß√µes fraudadas e leg√≠timas com o objetivo de compreender a din√¢mica das opera√ß√µes de compra e identificar semelhan√ßas e diferen√ßas marcantes entre os milhares de dados deste dataset.

Algumas observa√ß√µes e poss√≠veis melhorias para uma pr√≥xima etapa:

1 - Segmentar os dados por tipo de documenta√ß√£o entregue e olhar separadamente para caracter√≠sticas que diferenciam e assemelham os grupos. Desta forma poderia gerar novas colunas com estas caracter√≠sticas, que poderiam ser valiosas para uma modelagem posterior.

2 - Separar as compras por m√™s e analisar cada per√≠odo separadamente para identificar sazonalidades. Fazer feature engineering usando a coluna data para gerar novas colunas com semana do m√™s, dia da semana, final de semana e feriados. Compras acontecem mais em quais dias da semana? E as fraudes, s√£o mais frequentes no in√≠cio ou no final da semana? Fazer o mesmo por per√≠odo do dia: manh√£, tarde, noite e madrugada. Talvez essas novas features possam enriquecer o modelo de classifica√ß√£o.

3 - Algumas colunas do grupo Scores poderiam ser usadas para agrupar e analisar os conjuntos de fraude / n√£o fraude em separado. Por exemplo a coluna Score_1 que possui apenas 4 valores, que podem ser considerados como categorias. Tentar descobrir o que cada ‚Äúcategoria‚Äù desta poderia acrescentar de informa√ß√£o ao modelo.

4 - Poss√≠vel uso de transforma√ß√µes alg√©bricas para tentar aproximar de uma normal as distribui√ß√µes de algumas colunas score que possuem distribui√ß√µes muito enviesadas.

5 - Aplicar algum modelo n√£o supervisionado de clusteriza√ß√£o (K-Means, por exemplo) e usar os clusters gerados como mais uma feature informativa para o modelo.

6 - Utilizar a informa√ß√£o de probabilidade de fraude do modelo legado como mais uma informa√ß√£o que pode agregar valor aos modelos testados.
<br>
<br>

>
>
>**9\. Pr√≥ximos Passos**
>
>
<br>

Pretendo utilizar este mesmo dataset na pr√≥xima sprint de  Machine Learning e aprimorar o conjunto atrav√©s do uso de feature engineering, conforme id√©ias levantadas na se√ß√£o anterior, com o objetivo de ganhar mais informa√ß√£o que possa ser √∫til para o modelo de classifica√ß√£o.

Pretendo  investigar as estrat√©gias de balanceamento de dados como Smote, Undersampling e Oversampling e experimentar se elas realmente s√£o favor√°veis em modelos de classifica√ß√£o baseados em √°rvore e em gradiente.

Pretendo tamb√©m utilizar MFlow para gerir e avaliar o fluxo de experimenta√ß√£o dos modelos e das etapas de fine-tuning, chegando no final a um modelo com melhor capacidade de generaliza√ß√£o. Alguns dos modelos que pretendo aplicar incluem Random Forest, Lgbm e Xgboost. Possivelmente usar modelos ensemble, eventualmente Redes Neurais, enfim, explorar as diferentes possibilidades e ver quais se adequam melhor a este tipo de problema.

Ainda pretendo salvar o modelo treinado em formato .pkl  e montar um sistema de consulta em uma interface web, possivelmente Streamlit ligado a uma API, onde possa inserir dados e receber uma sa√≠da de acordo com o modelo. Provavelmente irei precisar de algum m√°quina virtual rodando um container docker que encapsule todas as depend√™ncias e vers√µes para que o modelo possa funcionar online.

Assim, al√©m do modelo de Machine Learning em si, pretendo chegar a uma tentativa de deploy simulando uma situa√ß√£o de uso real deste modelo, extrapolando a parte mais te√≥rica e inserindo um aspecto mais pr√°tico da utiliza√ß√£o de machine learning para responder problemas  reais de neg√≥cio.
<br>
<br>

>
>
>**10\. Conclus√£o**
>
>
<br>

Este trabalho de an√°lise de dados e boas pr√°ticas desta sprint intermedi√°ria (minha segunda de tr√™s sprints), foi muito interessante para acrescentar e aprimorar habilidades, tanto de programa√ß√£o python, como em teoria das distribui√ß√µes e pr√°ticas de visualiza√ß√£o de dados.

Das disciplinas contidas nesta sprint, a de visualiza√ß√£o de dados foi a que mais me despertou interesse, gostei muito dos exemplos e do material usado pela professora Simone. Ela ofereceu vasta bibliografia interessante sobre data viz e tamb√©m um referencial te√≥rico consistente.

A disciplina de An√°lise e pr√© processamento tamb√©m foi muito produtiva, a professora Tatiana Escovedo √© muito carism√°tica e tem uma did√°tica incr√≠vel. Como este tema de Machine Learning e Pr√©-Processamento √© um tema no qual j√° estou imerso h√° alguns anos, senti que poderia haver um pouco mais de complexidade nos exemplos (dataset √çris) de tratamento e transforma√ß√µes dos dados, por√©m compreendo que o curso precisa abranger um amplo espectro de  experi√™ncias e conhecimentos. Mesmo assim ouvi muitos relatos de colegas com dificuldade para compreender conceitos b√°sicos de ci√™ncia de dados.

A disciplina de Engenharia de Software, do professor Marcos Kalinowski apresentou um referencial te√≥rico fundamental para  quem trabalha na pr√°tica com equipes multidisciplinares de engenheiros de software, engenheiros de dados e cientistas de dados, como as metodologias √°geis e Solid, boas pr√°ticas de DevOps, CI / CD, etc. 

De modo geral o balan√ßo desta sprint foi bem positivo e espero que este trabalho corresponda ao esperado pelos professores. Agrade√ßo especialmente aos colegas professores Hugo Villamizar e Ant√¥nio Pedro pelas √≥timas orienta√ß√µes e discuss√µes, tanto em aulas online, quanto atrav√©s do canal do Discord. Agrade√ßo profundamente pela paci√™ncia e dedica√ß√£o com a qual conduziram estas sess√µes e conversas.
<br>

